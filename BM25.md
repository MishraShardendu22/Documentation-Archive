We break the formula into **pieces** and explain what each piece does.

Formula:

```
score(d, q) =
Σ over query terms t [
   IDF(t) *
   ( TF(t,d) * (k1 + 1) )
   ---------------------------------
   ( TF(t,d) + k1 * (1 - b + b * |d| / avgdl ) )
]
```

---

## 1. Σ over query terms

You loop over each word in the query.

Example query:

```
"machine learning"
```

Two terms:

```
t = machine
t = learning
```

You compute a score for each term and add them.

---

## 2. IDF(t)

```
IDF(t) = log( (N - df + 0.5) / (df + 0.5) )
```

Meaning:

Rare word → big weight
Common word → small weight

Purpose: reward informative words.

---

## 3. TF(t,d)

Number of times term t appears in document d.

Example:

```
TF = 5
```

Means word appears 5 times.

---

## 4. The TF scaling fraction

```
(TF * (k1 + 1)) /
(TF + k1 * (1 - b + b * |d| / avgdl))
```

This is the **core of BM25**.

It does THREE things:

1. TF saturation
2. Length normalization
3. Parameter control

---

### 4.1 Numerator

```
TF * (k1 + 1)
```

If k1 = 1.5

```
TF * 2.5
```

Boosts TF, but will be controlled by denominator.

---

### 4.2 Denominator

```
TF + k1 * (1 - b + b * |d| / avgdl)
```

Break inner part:

```
(1 - b + b * |d| / avgdl)
```

* |d| = document length
* avgdl = average document length

If document is longer than average → term gets penalized
If shorter → slight boost

Multiply by k1 → strength of penalty.

Then add TF.

---

## 5. TF Saturation Effect

Try values (assume denominator constant = C):

| TF | Score Growth |
| -- | ------------ |
| 1  | big jump     |
| 5  | smaller jump |
| 20 | tiny jump    |

Meaning:

First few occurrences matter most.
Repeating word 100 times does NOT 100× score.

This avoids spammy docs winning.

---

## 6. Length Normalization Effect

If document is very long:

```
|d| / avgdl > 1
```

Denominator increases → score decreases.

Long docs naturally contain many words → we compensate.

---

## 7. k1 Effect

* k1 → 0 → ignore TF, binary match
* k1 high → TF almost linear

Controls TF importance.

---

## 8. b Effect

* b = 0 → ignore document length
* b = 1 → full length normalization

Controls length penalty.

---

## 9. What the Whole Term Means

For each word:

```
importance of word
×
how strongly it appears
×
penalty for long documents
```

Then summed.

---

## 10. Intuition Version

For each query word:

> Is this word rare?
> Does this document contain it?
> How many times?
> Is the document suspiciously long?

Answer those mathematically → add up.

---

## 11. Minimal Mental Model

```
BM25 ≈ IDF * Saturated_TF * Length_Normalization
```

